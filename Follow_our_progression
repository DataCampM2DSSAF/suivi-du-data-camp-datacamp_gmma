""""""""""""
First week :
""""""""""""

Objectives :
	- Discover the main ideas about the project 
	- Use new tools : GitHub, Kaggle
	
Because we are not really familiar with GitHub and Kaggle, one of the first thing was to start using them.
Kaggle allows us to use all the data online. Moreover, some notebooks are available and useful to show us different ideas to approach the problem.
GitHub is useful to write code within a team, as each member can see the other's codes, add his own, add other files, for instance a notebook where one can find explanations about the code, the subject, ...

The data we have is about games used by children to evaluate their skill. 
The main objective is, according to these data, to evaluate as good they are by giving them a mark in 0, 1, 2 or 3.

"""""""""""""
Second week : 
"""""""""""""

Objectives :
	- Understand data -> what is the target, what are the different files, ...
	- Divide tasks -> what each team member have to do
	- First Kaggle submission -> at least one of us must submit a result
	- Set up Github -> make a clear difference between Readme, code and notebook
	
Because understanding data is more difficult than we thought at the first time, it is not so easy to still divide tasks.
We started to submit something on Kaggle just to see how Kaggle works, what we have to put in it.
Concerning the GitHub, we started to make the difference between Readme, code and notebook. However, some things are still not clear for us, so we will speak with our teacher next session.
The main problem we get is to get the accuracy_group, which is the mark for each child. It seems to be a big problem for every team. The good point is that some teams already gave their advices and code in some notebooks, and we can use them to try to solve this problem.
To get marks, we need to use words 'correct":true' or 'correct":false' for each game_session. These words are foud in 'event_data', a variable from the training and testing sets.

""""""""""""
Third week :
""""""""""""

Objectives :
	- Continue to understand how to get the accuracy in the train file
	- Each team member have to submit something on Kaggle
	
All of us have submit a random result on Kaggle. 
We kept to understand the meaning of the the words 'correct":true' and 'correct":false'. 
Indeed, they are not only used when someone fail or win a game, that is why to count the number of fails and wins by game session, we have to filter the data by game session.
It is the last problem we have to create a file in the asked format, e.g. a file such as train_labels.csv, where there is the game_session id, the number of fails and wins, the accuracy group, ...
After creating this file, we will have to start writing an explanation of data files, main variables, ... and clean the data.


Based on the issues previously presented, we wrote a function to build the labels in the train file. 
We selected the lines where the event_code was 4100 or 4110: they correspond to the final events of game_session.
Then we applied the groupby function to the event_session column.
Finaly we counted the numbers of correct and incorrect attempts: the information is available in the event_data column: 
if the string 'correct:true' is present  it's a success, if not it's a failure.

Two main isues:
	1. Into the lines with the 4100/4110 event_code there are still lines with no information about the results of the assessment in the event_data column.
	2. We have to predict on how many attemps every child will sucess an assignement. The issue is there are many game_session for one installation_id.
	Moreover, is this best to make the prediction for every account and every type of game ? 

"""""""""""""
Fourth week :
"""""""""""""

Some answers/ideas:
	1. For each child, we need to predict an accuracy by game. Then, depending on the assessment he's taking, we'd give the corresponding accuracy.
	2. Given the fact that a child can play at the same game several times, we must take into account all his different attemps. For instance by using a mean, median or balanced mean of results over time...
	3. We observed that the id_installation numbers in the test are not in the train. 
		The main objective is to predict the accuracy of a given assessment knowing what the child did before this assessment (video clips or games)

Do not forget to:
	Remove from the train the children who never took an assessment, because we won't be able to know his abilities.

To do list
	1. Finishing the make_labels function (which transform train into train_labels) and make sure it's usefull. It seems to be important but we have to precisely determine WHY.
	2. Writing a first part of the report on the game, the data structure and the objectives to make things clearer. 
	3. Data vizualisation:
		a. By games (distributions)
		b. By children (profils)
		c. Games + children over time
		d. Check correlations between games ? Can we make groups of games basing on how children pass or not certain assessments.
	4. Feature extraction: dealing with the information contained in the event_data

Task assigned:
 	- Samuel Amoyal 	: Feature extraction
	- Emile Mardoc 		: Writing part
	- Thomas Marcoux Pépin	: Data Vizualisation
	- Odélia Guedj 		: The make_labels function


"""""""""""""
Fifth week :
"""""""""""""

We wrote a function able to transform the train into the train_labels. It's usefull for 2 main reasons:
	First it allows us to better understand the data 
	Second we will have to apply this function to the test file: the goal of the competition is to predict the accuracy_group
	of children for a random assignment given their previous activities. So we will have to extract the accuracy group for every assignment
	done before the assignement chosen for prediction for every child.
	
A first section of the report was written. 



"""""""""""""
Sixth week :
"""""""""""""

Two main ideas where followed.

The first is just to continue to write the report (Emile) by using Thomas' Data Visualization.

The second is more complicated. Indeed, the function written by Samuel and Odélia to create the "train_labels" file by using "train" file works, but had to be improved.

	1) A mistake was in the Kaggle subject and was found by other Data Scientists. 
	Indeed, it was written that for the game "Bird", the code we have to use to find the "accuracy_group" is 4110, but sometimes it is 4100 too.
	We have corrected it, so our "train_labels" is not exactly the same than the file given in Kaggle competition.
	
	2) The function now works with the "test" file too.
	
	3) For some "installation_id" in the "test" file, there are just few clips, activities and the assessment we have to predict. 
	It means that we can't train only on "train_labels" because it only contains information about "assessment" and not about "clips","games" and "activities"
	
	4) The "assessment" we have to predict in the "test" file is the last of the assessments available, but it doesn't mean that it correspond to the last assessment made by a child.

	5) To train a model by using the "train" file, we need to modify this file in order for it to be like the "test" file.
	Because the main difference between these files is that in the "test" file, information about last assessment is missing, we have to truncate at least one "assessment" by "installation_id" int he "train" file.
	
TODO :

	1) Continue Data Visualization and the report (Emile and Thomas)
	2) Change "train" to be in the same format than "test" (Odélia and Emile)
	3) After that, do a training and submit it on Kaggle (Samuel)
